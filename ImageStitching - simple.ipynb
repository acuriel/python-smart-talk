{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OpenCV-ImageStitching.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cv",
      "language": "python",
      "name": "cv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2, imageio, imutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "cv2.ocl.setUseOpenCL(False)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# read images and transform them to grayscale\n",
        "# Make sure that the train image is the image that will be transformed\n",
        "trainImg = imageio.imread('http://www.ic.unicamp.br/~helio/imagens_registro/foto2A.jpg')\n",
        "trainImg_gray = cv2.cvtColor(trainImg, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "queryImg = imageio.imread('http://www.ic.unicamp.br/~helio/imagens_registro/foto2B.jpg')\n",
        "# Opencv defines the color channel in the order BGR. \n",
        "# Transform it to RGB to be compatible to matplotlib\n",
        "queryImg_gray = cv2.cvtColor(queryImg, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, constrained_layout=False, figsize=(16,9))\n",
        "ax1.imshow(queryImg, cmap=\"gray\")\n",
        "ax1.set_xlabel(\"Query image\", fontsize=14)\n",
        "\n",
        "ax2.imshow(trainImg, cmap=\"gray\")\n",
        "ax2.set_xlabel(\"Train image (Image to be transformed)\", fontsize=14)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def detectAndDescribe(image):\n",
        "    \"\"\"\n",
        "    Compute key points and feature descriptors using an specific method\n",
        "    \"\"\"\n",
        "    descriptor = cv2.ORB_create()\n",
        "        \n",
        "    # get keypoints and descriptors\n",
        "    (kps, features) = descriptor.detectAndCompute(image, None)\n",
        "    \n",
        "    return (kps, features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "kpsA, featuresA = detectAndDescribe(trainImg_gray)\n",
        "kpsB, featuresB = detectAndDescribe(queryImg_gray)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# display the keypoints and features detected on both images\n",
        "fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20,8), constrained_layout=False)\n",
        "ax1.imshow(cv2.drawKeypoints(trainImg_gray,kpsA,None,color=(0,255,0)))\n",
        "ax1.set_xlabel(\"(a)\", fontsize=14)\n",
        "ax2.imshow(cv2.drawKeypoints(queryImg_gray,kpsB,None,color=(0,255,0)))\n",
        "ax2.set_xlabel(\"(b)\", fontsize=14)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def createMatcher(crossCheck):\n",
        "    \"Create and return a Matcher Object\"\n",
        "    \n",
        "    return  cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=crossCheck)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def matchKeyPointsBF(featuresA, featuresB):\n",
        "    bf = createMatcher(crossCheck=True)\n",
        "        \n",
        "    # Match descriptors.\n",
        "    best_matches = bf.match(featuresA,featuresB)\n",
        "    \n",
        "    # Sort the features in order of distance.\n",
        "    # The points with small distance (more similarity) are ordered first in the vector\n",
        "    rawMatches = sorted(best_matches, key = lambda x:x.distance)\n",
        "    print(\"Raw matches (Brute force):\", len(rawMatches))\n",
        "    return rawMatches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(20,8))\n",
        "matches = matchKeyPointsBF(featuresA, featuresB)\n",
        "img3 = cv2.drawMatches(trainImg,kpsA,queryImg,kpsB,matches[:100], None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
        "\n",
        "plt.imshow(img3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def getHomography(kpsA, kpsB, featuresA, featuresB, matches, reprojThresh):\n",
        "    # convert the keypoints to numpy arrays\n",
        "    kpsA = np.float32([kp.pt for kp in kpsA])\n",
        "    kpsB = np.float32([kp.pt for kp in kpsB])\n",
        "    \n",
        "    if len(matches) > 4:\n",
        "\n",
        "        # construct the two sets of points\n",
        "        ptsA = np.float32([kpsA[m.queryIdx] for m in matches])\n",
        "        ptsB = np.float32([kpsB[m.trainIdx] for m in matches])\n",
        "        \n",
        "        # estimate the homography between the sets of points\n",
        "        (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC,\n",
        "            reprojThresh)\n",
        "\n",
        "        return (matches, H, status)\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "M = getHomography(kpsA, kpsB, featuresA, featuresB, matches, reprojThresh=4)\n",
        "if M is None:\n",
        "    print(\"Error!\")\n",
        "(matches, H, status) = M\n",
        "print(H)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply panorama correction\n",
        "width = trainImg.shape[1] + queryImg.shape[1]\n",
        "height = trainImg.shape[0] + queryImg.shape[0]\n",
        "\n",
        "result = cv2.warpPerspective(trainImg, H, (width, height))\n",
        "result[0:queryImg.shape[0], 0:queryImg.shape[1]] = queryImg\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.imshow(result)\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# transform the panorama image to grayscale and threshold it \n",
        "gray = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\n",
        "thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)[1]\n",
        "\n",
        "# Finds contours from the binary image\n",
        "cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "cnts = imutils.grab_contours(cnts)\n",
        "\n",
        "# get the maximum contour area\n",
        "c = max(cnts, key=cv2.contourArea)\n",
        "\n",
        "# get a bbox from the contour area\n",
        "(x, y, w, h) = cv2.boundingRect(c)\n",
        "\n",
        "# crop the image to the bbox coordinates\n",
        "result = result[y:y + h, x:x + w]\n",
        "\n",
        "# show the cropped image\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.imshow(result)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}